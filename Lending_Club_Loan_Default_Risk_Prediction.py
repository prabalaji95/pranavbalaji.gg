# -*- coding: utf-8 -*-
"""Simple_Neural_Network_Lending_Club_Loan_Default_Risk_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bRjpzuLqINGzvkx3_Si8rkKsLvZ1YeMi

#Business Background
LendingClub, based in San Francisco, California, is a pioneering peer-to-peer lending company in the US. It set a precedent by becoming the first of its kind to register its services as securities with the Securities and Exchange Commission (SEC) and introduced loan trading on a secondary market. Today, it stands as the globe's premier peer-to-peer lending platform.

You are part of the LendingClub team, a company that offers diverse loan options to urban clients. Whenever a loan request is submitted, LendingClub must evaluate the applicant's profile to make an informed loan approval decision. The outcome hinges on two potential risks:

* Denying a loan to an applicant who is capable of repayment means missed business opportunities for LendingClub.
* Conversely, approving a loan to an applicant prone to defaulting can spell financial setbacks for the company.

The provided dataset encompasses historical data on loan applicants, highlighting who defaulted and who didn't. The goal is to discern patterns that signal the likelihood of an applicant defaulting. Such insights can guide strategies like denying the loan, adjusting the loan amount, or setting higher interest rates for riskier borrowers.

Curated from https://www.kaggle.com/code/faressayah/lending-club-loan-defaulters-prediction/notebook by Fares Sayah

# Settings
"""

# ! pip install -q shap

import pandas as pd
import numpy as np
import seaborn as sns
from scipy import stats
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import MinMaxScaler

from sklearn.metrics import (
    accuracy_score, confusion_matrix, classification_report,
    roc_auc_score, roc_curve, auc,
)
from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay
from sklearn.ensemble import RandomForestClassifier

import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import AUC
from tensorflow.keras.metrics import Recall

pd.set_option('display.float', '{:.2f}'.format)
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 50)

"""# Load Data

Note: the data has been cleaned up and transformed. To learn more about the original data, check here
https://www.kaggle.com/code/faressayah/lending-club-loan-defaulters-prediction/notebook
"""

data_url = 'https://raw.githubusercontent.com/JHU-CDHAI/Dataset/main/lending_club_loan_processed.csv'

data = pd.read_csv(data_url)
print(data.shape)
data.head()

# 0: means Fully Paid
# 1: means Charged Off
data['loan_status'].value_counts()

data.head()

data.columns

data.shape

# Comment this out if you want to use Zip Code information.
new_cols = [i for i in data.columns if 'zip' not in i]
data = data[new_cols]

data.columns

"""# Data preparation"""

train, test = train_test_split(data, test_size=0.33, random_state=42)

print(train.shape)
print(test.shape)

# (264796, 81)
# (130423, 81)

# Removing outliers
print(train.shape)
train = train[train['annual_inc'] <= 250000]
train = train[train['dti'] <= 50]
train = train[train['open_acc'] <= 40]
train = train[train['total_acc'] <= 80]
train = train[train['revol_util'] <= 120]
train = train[train['revol_bal'] <= 250000]
print(train.shape)

# Normalizing the data
X_train, y_train = train.drop('loan_status', axis=1), train['loan_status']
X_test,  y_test  = test.drop('loan_status', axis=1),  test['loan_status']

# y_train

scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

X_train = np.array(X_train).astype(np.float32)
X_test  = np.array(X_test).astype(np.float32)
y_train = np.array(y_train).astype(np.float32)
y_test  = np.array(y_test).astype(np.float32)

"""# Simple Neural Network model

## Build Model
"""

# Build a simple neural network model
model = Sequential()
#Add dense layer with 128 nodes. Normalize output, no regularization
model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0))
#Add dense layer with 128 nodes. Normalize output, 10% dropout rate to control overfitting
model.add(Dense(128, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.1))
#Add dense layer with 128 nodes. Normalize output, no regularization
model.add(Dense(128, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0))
#Add dense layer with 128 nodes. Normalize output, 10% dropout rate to control overfitting
model.add(Dense(128, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.1))
model.add(Dense(1, activation='sigmoid'))  # Binary classification, so use sigmoid activation

"""## Train Model"""

# Compile the model; adjusted learning rate to 0.0005 for additional precision
model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate = 0.0005), metrics=[Recall(), 'accuracy'])

# Train the model, adjusted epochs to 30 and batch size to 32
history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)

# Plot the training and validation loss
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.show()

"""## Evaluate Model"""

# assuming 'model' is your trained model
test_loss, test_recall, test_accuracy = model.evaluate(X_test, y_test, verbose=2)
print(f'Test accuracy: {test_accuracy}')
print(f'Test recall: {test_recall}')

y_pred = (model.predict(X_test) > 0.5).astype(int)

"""## Confusion Matrix"""

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
cm

from sklearn.metrics import precision_score, recall_score

# Assuming y_pred and y_test are numpy arrays or lists containing binary labels (0 or 1)

precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

print("Precision:", precision)
print("Recall:", recall)

plt.figure(figsize=(10,7))
sns.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Truth')


